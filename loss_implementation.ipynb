{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymQvcNlxxtIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "0e93787e-bbeb-4102-c008-03e45073c208"
      },
      "source": [
        "\"\"\"\n",
        "Q3. Implement Numerically correct versions of the following functions:\n",
        "\n",
        "\"\"\"\n",
        "import math\n",
        "import sys\n",
        "\n",
        "def part1_taylor_series(x):\n",
        "  \"\"\"\n",
        "  L(x) = log(1 + exp(-x))\n",
        "  \"\"\"\n",
        "  sum = 0\n",
        "  for i in range(50):\n",
        "    sum += np.exp((i+1) * x * (-1))/(i + 1)\n",
        "  return sum\n",
        "\n",
        "def part1_normal(x):\n",
        "  \"\"\"\n",
        "  L(x) = log(1 + exp(-x))\n",
        "  \"\"\"\n",
        "  if x >= 0:\n",
        "    return np.log(1 + np.exp(-1*x))\n",
        "  else:\n",
        "    return np.log(np.exp(x) + 1) - x\n",
        "    \n",
        "def part2(x1, x2):\n",
        "  \"\"\"\n",
        "  L(x) = log(exp(x1) + exp(x2))\n",
        "  \"\"\"\n",
        "  expval = math.exp(x2-x1)\n",
        "  sum = x1 + math.log(1 + expval)\n",
        "  return sum\n",
        "\n",
        "def part3(x1, x2):\n",
        "  \"\"\"\n",
        "  L(x) = exp(x1)/(exp(x1)+exp(x2))\n",
        "  \"\"\"\n",
        "  expval = math.exp(x2 - x1)\n",
        "  result = 1.0/(expval + 1)\n",
        "  return result\n",
        "\n",
        "x = 500\n",
        "x1 = 100\n",
        "x2 = 11\n",
        "y1 = part1_taylor_series(x)\n",
        "y1_1 = part1_normal(x)\n",
        "y2 = part2(x1,x2)\n",
        "y3 = part3(x1,x2)\n",
        "print(y1)\n",
        "print(y1_1)\n",
        "print(y2)\n",
        "print(y3)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.124576406741286e-218\n",
            "0.0\n",
            "100.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB5RG9bzs3U_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Q4. Implement the Following Loss Functions in Python. Implement these functions as e\u000e-\n",
        "ciently as you can (i.e. using vectorized code). Verify the implementations with a simple For Loop\n",
        "based implement. Finally, verify the computation of the Gradients with a numerical implement of\n",
        "the Gradients.\n",
        "\n",
        "Outputs : The first value returned is the loss and an array of gradient\n",
        "Study : Vectorized version are faster and work much better than the for loop version and results are same.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def numericalGrad(funObj, w, epsilon):\n",
        "  m = len(w)\n",
        "  gradient = np.zeros(m)\n",
        "  for i in range(m):\n",
        "    wp = np.copy(w)\n",
        "    wn = np.copy(w)\n",
        "    wp[i] = w[i] + epsilon\n",
        "    wn[i] = w[i] - epsilon\n",
        "    gradient[i] = (funObj(wp) - funObj(wn))/(2*epsilon)\n",
        "  \n",
        "  return gradient\n",
        "  \n",
        "def generateInstance(num_features):\n",
        "  instance = []\n",
        "  for i in range(num_features):\n",
        "    instance.append(random.randint(0,1))\n",
        "  return np.array(instance)\n",
        "\n",
        "def generateDataset(num_instances, num_features):\n",
        "  dataset = pd.DataFrame([generateInstance(num_features) for _ in range(num_instances)])\n",
        "  return np.array(dataset)\n",
        "\n",
        "def generateLabels(num_instances, labels):\n",
        "  n_lables = []\n",
        "  for i in range(num_instances):\n",
        "    index = random.randint(0, len(labels)-1)\n",
        "    n_lables.append(labels[index])\n",
        "  return np.array(n_lables)\n",
        "\n",
        "def logisticLossFunc(w, X, y, lam):\n",
        "  m = X.shape[0]\n",
        "  loss = 0\n",
        "  for i in range(m):\n",
        "    var = -1 * y[i] * np.dot(w, X[i])\n",
        "    #print(var)\n",
        "    loss = loss + np.log(1 + np.exp(var))\n",
        "  loss += lam*(np.linalg.norm(w)**2)\n",
        "\n",
        "  grad = np.zeros(w.shape)\n",
        "  for i in range(X.shape[0]):\n",
        "    grad += ((-1)* np.exp(-1*y[i]*np.dot(w, X[i]))/(1 + np.exp(-1*y[i]*np.dot(w, X[i]))))*y[i]*X[i]\n",
        "\n",
        "  grad += 2*lam*w\n",
        "  return loss, grad\n",
        "\n",
        "def logisticLossVectorizedFunc(w, X, y, lam):\n",
        "  m = X.shape[0]\n",
        "  Xw = X.dot(w)\n",
        "  \n",
        "  loss = np.sum(np.log(1 + np.exp(-1* y * Xw)))\n",
        "  loss += lam*(np.linalg.norm(w)**2)\n",
        "\n",
        "  grad = np.zeros(w.shape)\n",
        "  nexp = np.exp(-1 * y * Xw)\n",
        "  \n",
        "  grad = (-1) * np.matmul(X.T, y * nexp / (1 + nexp))\n",
        "  #for i in range(X.shape[0]):\n",
        "  #  grad += ((-1)* np.exp(-1*y[i]*np.dot(w, X[i]))/(1 + np.exp(-1*y[i]*np.dot(w, X[i]))))*y[i]*X[i]\n",
        "\n",
        "  grad += 2*lam*w\n",
        "  return loss, grad\n",
        "\n",
        "def hingeLossFunc(w, X, y, lam):\n",
        "  m = X.shape[0]\n",
        "  loss = 0\n",
        "  for i in range(m):\n",
        "    var = 1 - y[i] * np.dot(w, X[i])\n",
        "    loss += max(0, var)\n",
        "  loss += lam*(np.linalg.norm(w)**2)\n",
        "\n",
        "  grad = np.zeros(w.shape)\n",
        "  for i in range(X.shape[0]):\n",
        "    var = 1 - y[i] * np.dot(w, X[i])\n",
        "    if (var > 0):\n",
        "      grad += (-1)*y[i]*X[i]\n",
        "\n",
        "  grad += 2*lam*w\n",
        "  return loss, grad\n",
        "\n",
        "def hingeLossVectorizedFunc(w, X, y, lam):\n",
        "  m = X.shape[0]\n",
        "  Xw = X.dot(w)\n",
        "  tmp = (1 - 1* y * Xw)\n",
        "  idx = np.where(tmp > 0)\n",
        "  loss = np.sum(tmp[idx])\n",
        "  loss += lam*(np.linalg.norm(w)**2)\n",
        "\n",
        "  grad = np.zeros(w.shape)\n",
        "  rows = X.shape[0]\n",
        "  cols = X.shape[1]\n",
        "  grad = np.sum(np.where(np.repeat((1 - y*np.matmul(X, w)), cols).reshape(rows, cols) > 0, \n",
        "                         (-1)*np.multiply(np.repeat(y, cols).reshape(rows, cols), \n",
        "                                          X), 0), axis = 0)\n",
        "\n",
        "  grad = 2*lam*w + grad\n",
        "  return loss, grad\n",
        "\n",
        "def simple2LayerLossFunc(w, X, y, lam):\n",
        "  m = X.shape[0]\n",
        "  loss = 0\n",
        "  for i in range(m):\n",
        "    var = (y[i] - max(0, np.dot(w, X[i])))**2\n",
        "    loss += var\n",
        "  loss += lam*(np.linalg.norm(w)**2)\n",
        "\n",
        "  grad = np.zeros(X.shape[1])\n",
        "  for i in range(X.shape[0]):\n",
        "    cond = np.dot(w, X[i]) \n",
        "    if cond > 0:\n",
        "      grad += 2*(y[i] - np.dot(w, X[i]))*(-1*X[i])\n",
        "\n",
        "  grad += 2*lam*w\n",
        "  return loss, grad\n",
        "\n",
        "def simple2LayerLossVectorizedFunc(w, X, y, lam):\n",
        "  Xw = X.dot(w)\n",
        "  idx = np.where(Xw < 0)\n",
        "  Xw[idx] = 0\n",
        "  loss = np.sum((y - Xw) ** 2)\n",
        "\n",
        "  #for i in range(m):\n",
        "  #  var = (y[i] - max(0, np.dot(w, X[i])))**2\n",
        "  #  loss += var\n",
        "  loss += lam*(np.linalg.norm(w)**2)\n",
        "\n",
        "  rows = X.shape[0]\n",
        "  cols = X.shape[1]\n",
        "  grad = np.sum(np.where(np.repeat(np.matmul(X,w), cols).reshape(rows, cols) > 0,\n",
        "                         (-2)*np.multiply(np.repeat(y - np.matmul(X,w),cols).reshape(rows,cols), \n",
        "                                          X), 0), axis = 0)\n",
        "  grad += 2*lam*w\n",
        "  return loss, grad\n",
        "\n",
        "def leastSquareLoss(w, X, y, lam):\n",
        "  m = X.shape[0]\n",
        "  loss = 0\n",
        "  for i in range(m):\n",
        "    var = (y[i] - np.dot(w, X[i]))**2\n",
        "    loss += var\n",
        "  loss += lam*(np.linalg.norm(w)**2)\n",
        "\n",
        "  grad = np.zeros(X.shape[1])\n",
        "  for i in range(X.shape[0]):\n",
        "    grad += 2 * (-1) * (y[i] - np.dot(w, X[i])) * X[i]\n",
        "\n",
        "  grad += 2*lam*w\n",
        "  return loss, grad\n",
        "\n",
        "def leastSquareLossVectorized(w, X, y, lam):\n",
        "  Xw = X.dot(w)\n",
        "  loss = np.sum((y - Xw) ** 2)\n",
        "  loss += lam*(np.linalg.norm(w)**2)\n",
        "\n",
        "  grad = 2*np.matmul(y - Xw, -1*X)\n",
        "  grad += 2*lam*w\n",
        "  return loss, grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHxRk4PMsqcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "4396f434-0769-4dbf-8665-cf820c7c9abf"
      },
      "source": [
        "dataset = generateDataset(100, 10)\n",
        "labels = [-1, 1]\n",
        "n_labels = generateLabels(100, labels)\n",
        "#lr = LogisticRegression(C=1e5)\n",
        "#lr.fit(dataset, n_labels)\n",
        "w = np.random.random(X.shape[1])\n",
        "X = dataset\n",
        "Y = n_labels.T\n",
        "\n",
        "epsilon = 1e-15\n",
        "lam = 0.01\n",
        "\n",
        "print(\"LogisticLossforLoop\")\n",
        "output = logisticLossFunc(w, X, Y, lam)\n",
        "print(output)\n",
        "\n",
        "print(\"logisticLossVectorized\")\n",
        "output = logisticLossVectorizedFunc(w, X, Y, lam)\n",
        "print(output)\n",
        "\n",
        "print(\"hingeLossforLoop\")\n",
        "output = hingeLossFunc(w, X, Y, lam)\n",
        "print(output)\n",
        "\n",
        "print(\"hingeLossVectorized\")\n",
        "output = hingeLossVectorizedFunc(w, X, Y, lam)\n",
        "print(output)\n",
        "\n",
        "print(\"simple2LayerLossForLoop\")\n",
        "output = simple2LayerLossFunc(w, X, Y, lam)\n",
        "print(output)\n",
        "\n",
        "print(\"simple2LayerLossVectorized\")\n",
        "output = simple2LayerLossVectorizedFunc(w, X, Y, lam)\n",
        "print(output)\n",
        "\n",
        "Y = np.arange(0, 1, 0.01)\n",
        "print(\"leastSquareLossForloop\")\n",
        "output = leastSquareLoss(w, X, Y, lam)\n",
        "print(output)\n",
        "\n",
        "print(\"leastSquareLossVectorized\")\n",
        "output = leastSquareLossVectorized(w, X, Y, lam)\n",
        "print(output)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticLossforLoop\n",
            "(158.45072794450635, array([22.73623606, 27.14577845, 24.43548614, 21.99006437, 28.89196318,\n",
            "       29.33555428, 26.71625344, 22.97124311, 23.99027074, 19.09597247]))\n",
            "logisticLossVectorized\n",
            "(158.4507279445064, array([22.73623606, 27.14577845, 24.43548614, 21.99006437, 28.89196318,\n",
            "       29.33555428, 26.71625344, 22.97124311, 23.99027074, 19.09597247]))\n",
            "hingeLossforLoop\n",
            "(201.15004013781692, array([25.01964659, 29.01370143, 28.01030709, 25.01198536, 32.0154558 ,\n",
            "       32.00918747, 32.00404646, 26.01633356, 27.0004795 , 22.00408399]))\n",
            "hingeLossVectorized\n",
            "(201.15004013781692, array([25.01964659, 29.01370143, 28.01030709, 25.01198536, 32.0154558 ,\n",
            "       32.00918747, 32.00404646, 26.01633356, 27.0004795 , 22.00408399]))\n",
            "simple2LayerLossForLoop\n",
            "(960.5187857601481, array([319.44012197, 311.84049902, 302.93436534, 281.73694755,\n",
            "       358.30543237, 351.27607589, 309.06440423, 301.30177304,\n",
            "       295.43752502, 249.42072142]))\n",
            "simple2LayerLossVectorized\n",
            "(960.5187857601484, array([319.44012197, 311.84049902, 302.93436534, 281.73694755,\n",
            "       358.30543237, 351.27607589, 309.06440423, 301.30177304,\n",
            "       295.43752502, 249.42072142]))\n",
            "leastSquareLossForloop\n",
            "(570.7250914079655, array([270.66012197, 239.82049902, 242.55436534, 230.27694755,\n",
            "       289.68543237, 272.79607589, 234.98440423, 242.92177304,\n",
            "       235.55752502, 199.30072142]))\n",
            "leastSquareLossVectorized\n",
            "(570.7250914079657, array([270.66012197, 239.82049902, 242.55436534, 230.27694755,\n",
            "       289.68543237, 272.79607589, 234.98440423, 242.92177304,\n",
            "       235.55752502, 199.30072142]))\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}